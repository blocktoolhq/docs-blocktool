---
sidebar_position: 3
---

# Stream Completion

Given a chat conversation, the model will stream a chat completion response, one chunk at a time. This is ideal for production usage where the user is waiting on a response.

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
<TabItem value="ts" label="TypeScript">

```ts
import { Blocktool } from 'blocktool';
const blocktool = new Blocktool('<YOUR API KEY>');

async function main() {
  prompt = {
    messages: [
      {role: "system", content: "You are a helpful assistant."},
      {role: "user", content: "What is the capital of Paris?"},
    ],
  };
  await blocktool.streamCompletion({
    model: "claude-v1.2",
    max_tokens: 64,
    prompt: prompt,
    temperature: 0.2,
  }, (chunk) => {
    console.log(chunk.text);
  });
}
```

</TabItem>
<TabItem value="py" label="Python">

```py
import asyncio
from blocktool import Blocktool, Prompt

blocktool = Blocktool('<YOUR API KEY>')

async def main():
    prompt = Prompt(
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the capital of Paris?"},
        ]
    )
    async for chunk in blocktool.stream_completion(
        model="claude-v1.2",
        max_tokens=64,
        prompt=prompt,
        temperature=0.2,
    ):
        print(chunk.text or '')

if __name__ == "__main__":
    asyncio.run(main())
```

</TabItem>
</Tabs>